---
title: "Document Classification"
author: "Keeno Glanville"
date: "2022-11-20"
output: html_document
---
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).
```{r setup, include=FALSE}
set.seed(1234)
library(tidyverse) 
library(DT)
library(knitr)
library(dplyr)
library(tm)
library(wordcloud)
library(caret)
```
Within this Project I will be utilizing the ham and spam folders downloaded from
the "Index of /old/publiccorpus" website linked :https://spamassassin.apache.org/old/publiccorpus/

As these files are too large to open in raw contents on github, they will be input
from my local files, however I will provide a the source files in a zipped folder
within the repository of this project.


# Load Data
```{r}
spam_path<- "C:/Users/keeno/OneDrive/Documents/MSDS/DATA607/Document Classification/spam_2"
ham_path <- "C:/Users/keeno/OneDrive/Documents/MSDS/DATA607/Document Classification/easy_ham"
spam_filenames <- list.files(spam_path)
ham_filenames <- list.files(ham_path)
spam_filenames <- spam_filenames[which(spam_filenames!="cmds")]
ham_filenames <- ham_filenames[which(ham_filenames!="cmds")]


head(ham_filenames)
head(spam_filenames)
```

#Process text data with corpus


```{r}
hamcorpus<- ham_path %>%
  paste(., list.files(.), sep = "/") %>%
  lapply(readLines) %>%
  VectorSource() %>%
  VCorpus()%>%
  tm_map(removeWords, stopwords()) %>%
  tm_map(stripWhitespace) %>%
  tm_map(stemDocument)%>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation)
  

spamcorpus <- spam_path %>%
  paste(., list.files(.), sep = "/") %>%
  lapply(readLines) %>%
  VectorSource() %>%
  VCorpus()%>%
  tm_map(removeWords, stopwords()) %>%
  tm_map(stripWhitespace) %>%
  tm_map(stemDocument)%>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation)
  


```

# Combine Spam and Ham to build Matrix


```{r}
df_H <- as.data.frame(unlist(hamcorpus), type = "ham" , stringsAsFactors = FALSE)
df_H$type <- "ham"
colnames(df_H)=c("text", "spam")

df_S <- as.data.frame(unlist(spamcorpus), type = "spam" , stringsAsFactors = FALSE)
df_S$type <- "spam"
colnames(df_S)=c("text", "spam")

#Dataframe
df_HS <- rbind(df_H, df_S)
df_HS[df_HS == "spam"] <- 1
df_HS[df_HS == "ham"] <- 0
View(df_HS)


dtm<- DocumentTermMatrix(hamcorpus)
spdtm <- removeSparseTerms(dtm, 0.95)

emailsSparse <- as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) <- make.names(colnames(emailsSparse))
sort(colSums(emailsSparse))



```

# DataFrame Creation        
```{r}
df_H <- as.data.frame(unlist(hamcorpus), type = "ham" , stringsAsFactors = FALSE)
df_H$type <- "ham"
colnames(df_H)=c("text", "email")

df_S <- as.data.frame(unlist(spamcorpus), type = "spam" , stringsAsFactors = FALSE)
df_S$type <- "spam"
colnames(df_S)=c("text", "email")

#Dataframe
df_HS <- rbind(df_H, df_S)



```
# Training Data Frame for Prediction

```{r}

df_HS$id <- 1:nrow(df_HS)

#Use 70% of dataset as training set and remaining 30% as testing set 
train <- df_HS %>% dplyr::sample_frac(0.7)
test  <- dplyr::anti_join(df_HS, train, by = 'id')



```
# Corpus Creation

```{r}


```

#Prediction

```{r}

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

log_model <- train(email ~ text , data = train_tdm,
                 method="glm", family = binomial(link = "logit"),
                 trControl = fitControl)

train_preds <- predict(log_model,newdata = trainset)
test_preds <- predict(log_model,newdata = testset)
train_preds <- predict(log_model,newdata = trainset)
test_preds <- predict(log_model,newdata = testset)
```
visualization


```{r}
wordcloud(hamspamcorpus, max.words = 100, random.order = FALSE, rot.per=0.15, min.freq=5, colors = brewer.pal(8, "Dark2"))


```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```



